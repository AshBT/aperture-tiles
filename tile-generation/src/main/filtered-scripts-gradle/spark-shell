#!/bin/bash

# Make sure SPARK_HOME is set
if [ "a" == "a""$SPARK_HOME" ]; then
  echo SPARK_HOME not set.  Please set SPARK_HOME environment variable and try again.
  exit 1
fi

# Make sure SCALA_HOME is set
if [ "a" == "a""$SCALA_HOME" ]; then
  echo SCALA_HOME not set.  Please set SCALA_HOME environment variable and try again.
  exit 1
fi

function getJar {
	scriptGroupId=$1
	scriptArtifactId=$2
	scriptVersion=$3
	groupDir=`echo ${scriptGroupId} | sed 's|\.|/|g'`
	repo=${HOME}/.m2/repository
	jardir=${repo}/${groupDir}/${scriptArtifactId}/${scriptVersion}
	JAR=${jardir}/${scriptArtifactId}-${scriptVersion}.jar

	if [ ${OSTYPE} == "cygwin" ]; then
		export JAR="$(cygpath -p -w "${JAR}")"
	fi
}

function addSparkJar {
	getJar $1 $2 $3
	jar=${JAR}

	# Testing for existence of jar
	if [ ! -e ${jar} ]
	then
		echo Error: Looking for ${jar} - not found
		exit
	fi

	# Add the jar to the jar list
	if [ "a" == "a"${SPARK_JARS} ]; then
		export SPARK_JARS=${jar}
	else
		export SPARK_JARS=${SPARK_JARS},${jar}
	fi
}


# Set up the list of extra jars needed
# our own jars
addSparkJar com.oculusinfo math-utilities @project.version@
addSparkJar com.oculusinfo binning-utilities @project.version@
addSparkJar com.oculusinfo tile-generation @project.version@

# framework-related jars
addSparkJar org.apache.hbase hbase @hbase-version@
addSparkJar org.scala-lang scala-library @scala-version@
# json version is derived indirectly, can't be coded here.
addSparkJar org.json json 20090211




echo Running Spark from $SPARK_HOME
echo Running Spark with jars ${SPARK_JARS}

$SPARK_HOME/bin/spark-shell --jars ${SPARK_JARS} $*
