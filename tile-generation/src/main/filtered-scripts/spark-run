#!/bin/bash

# Make sure SPARK_HOME is set
if [ "a" == "a""$SPARK_HOME" ]; then
  echo SPARK_HOME not set.  Please set SPARK_HOME environment variable and try again.
  exit 1
fi

# Make sure SCALA_HOME is set
if [ "a" == "a""$SCALA_HOME" ]; then
  echo SCALA_HOME not set.  Please set SCALA_HOME environment variable and try again.
  exit 1
fi

# export SPARK_CLASSPATH=$SCALA_HOME/libexec/lib/'*'

function addToSparkClasspath {
  scriptGroupId=$1
  scriptArtifactId=$2
  scriptVersion=$3
  groupDir=`echo ${scriptGroupId} | sed 's|\.|/|g'`
  repo=${HOME}/.m2/repository
  jardir=${repo}/${groupDir}/${scriptArtifactId}/${scriptVersion}
  jar=${jardir}/${scriptArtifactId}-${scriptVersion}.jar

  # Testing for existence of jar
  if [ ! -e ${jar} ]
  then
      echo Error: Looking for ${jar} - not found
      exit
  fi

  # Add the jar to the classpath
  if [ "a" == "a"${SPARK_CLASSPATH} ]; then
    export SPARK_CLASSPATH=${jar}
  else
    export SPARK_CLASSPATH=${SPARK_CLASSPATH}:${jar}
  fi
}






# Set up the spark classpath
# our own jars
addToSparkClasspath com.oculusinfo math-utilities ${project.version}
addToSparkClasspath com.oculusinfo binning-utilities ${project.version}
addToSparkClasspath com.oculusinfo tile-generation ${project.version}

# framework-related jars
addToSparkClasspath org.apache.hbase hbase ${hbase-version}
addToSparkClasspath org.scala-lang scala-library ${scala-version}
#json version is derived indirectly, can't be coded here.
addToSparkClasspath org.json json 20090211



# export MASTER=local[8]
# export SPARK_MEM=4g

echo Running Spark from $SPARK_HOME
echo Running Spark on $MASTER
echo Running Spark with $SPARK_MEM
echo Running Spark with classpath=$SPARK_CLASSPATH

$SPARK_HOME/bin/spark-class $*
