# spark connection details - specify the url of your spark master, connection
# user and location of spark home directory


# spark.connection.url=
# spark.connection.home=

# used to help distingish spark jobs in the spark or mesos UI, so other users
# can see who is running the job

spark.connection.user=mkielo

# output files to the current directory of the filesystem, and give them a
# prefix, to distinguish them from other runs
oculus.tileio.type=file
oculus.binning.prefix=test

oculus.binning.output.location=output/fpSamp.txt


# Name of the binning dataset
oculus.binning.name=rdns
# location of raw input data - this can be a single file, or a directory full
# of files (which is standard for Spark).  If a directory, all files in the
# directory will be read.
# oculus.binning.source.location=hdfs://hadoop-s1:8020//user/mkielo/net-scan/fingerprints/fpsample1000.txt
 oculus.binning.source.location=c:/Users/mkielo/workspace/experiments/net-scan/fingerprints1000sample.txt

#character used as a field delimiter
oculus.binning.parsing.separator=\t

# Define which tests to run on the entire table
oculus.binning.table.tests=totalRecords

# Define the fields to parse in the CSV files. Starting from 0
# oculus.binning.parsing.time.index=1
# oculus.binning.parsing.time.fieldType=numerical
# oculus.binning.parsing.time.tests=

# oculus.binning.parsing.ip.index=0
# oculus.binning.parsing.ip.fieldType=qualitative
# oculus.binning.parsing.ip.tests=

oculus.binning.parsing.ip.index=2
oculus.binning.parsing.ip.fieldType=qualitative
oculus.binning.parsing.ip.tests=
oculus.binning.parsing.fp.custom.analytics=fingerprints
oculus.binning.parsing.fp.custom.variables=
oculus.binning.parsing.fp.custom.output=output/customAnalytic/FP.txt